# General Structure
I think I should have a section where I talk about the methods I used so many times that failed. Its sort of important to frame my successes and failures, in part to show how much I did, and in part to show what I learned. 



Description of the winning algorithm:
The winning algorithm is a series of iterations between a value-network and a policy network. First, describe how each of them are used.

A policy network goes from an input, which is a breakdown of the board plus some simple game-features, to a map of the board that is the probability the person should make that move. To pick the best move, you pick the legal move with the highest probability. To introduce some randomness, you pick a legal move in proportion to its probability.

A value-network turns a board-input into a single number, between -1 and 1, which determines whether the network thinks that that player (given optimal play on both sides) will win or will lose. To pick a move, from a board_position you figure out all valid moves, you simulate each of them, and then you choose the one that creates the board with the highest value.

The first pass of the value network is to figure out board-values given random play from that point on. To do this, play an entire random game, determine the winner, then extract a random board from the series of this game. I serialized 250,000 of these results, meaning the BOARD, the TURN for that board, and the OUTCOME.

From the value network, you can train a policy-network. The probability of a move should be proportional to how well the value of the resulting board is compared to the values of the other possible resulting boards. A policy target can be created which is a matrix of the values of each move, passed through a softmax function. Decreasing the softmax-temperature makes the policy-function more selective for high-value moves.

The first run through, I used the randomly-generated boards to train the policy network. The remaining runs through, I created boards through complete self-play (in which both sides try and pick near-optimal moves) in order to weigh the distribution towards competetive board-positions.

Repeating this over and over causes the value function to become more and more precise. Then, in actualy play, you choose the best move each time.



Convergence: 


Possible pitfalls:





It is sensible that one would like to go in proportion to the value the 

This actually created a decently strong GO player. To make it stronger, we want access to better estimates of the value of a board. 



BUT, the values that it was trained off of were generated by simulating random play.


Honestly, there's no reason for the value function at all. I don't really understand why I did that. It's WAY slower. I guess they needed theirs for their rollout, but not I.


